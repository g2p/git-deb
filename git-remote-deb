#!/usr/bin/python3.3
"""
deb:pkgname

Sadly, git parses the above as ssh pseudo-urls.
Workaround:
deb::deb:pkgname deb::pkgname
"""

import argparse
import chardet # pypi
import collections
import contextlib
import debian.deb822 # pypi:python-debian
import hashlib
import io
import isodate # pypi
import itertools
import json
import os
import re
import requests # pypi
import subprocess
import sys
import tempfile
import time
import types
import urllib.parse

import gitdeb


SNAPSHOTS_BASE = 'http://snapshot.debian.org/'
# Don't write to ~/.local/share/keyrings,
# it's where gnome-keyrings stores secrets.
KEYRINGS_PATH = (
    os.path.expanduser('~/.local/share/public-keyrings'),
    os.path.expanduser('~/.local/share/keyrings'), '/usr/share/keyrings')

IDENT_RE = re.compile(r'^([^<>]*?)\s*<([^<>]+)>$')
SIG_KID_RE = re.compile(r'^[0-9A-F]{16}$')

# I've no idea
CHUNK_SIZE = 65536

# Yay VT100
LINE_START = '\r'
CLEAR_END_OF_LINE = '\x1b[K'


class JSONEncoder(json.JSONEncoder):
    def default(self, o):
        if isinstance(o, types.SimpleNamespace):
            return vars(o)
        return super().default(o)

def json_dump(val):
    return json.dumps(val, cls=JSONEncoder)

def silent_call(cmd):
    try:
        subprocess.check_output(cmd, stderr=subprocess.STDOUT)
    except subprocess.CalledProcessError as exn:
        bail('Command {} failed with output {}'.format(cmd, exn.output))

def ignore(*args):
    pass

def printerr(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)

warn = printerr
debug = ignore

def bail(msg):
    printerr(msg)
    sys.exit(1)

if sys.stderr.isatty():
    def progress(msg):
        printerr(CLEAR_END_OF_LINE + msg, end=LINE_START)
else:
    progress = ignore


class BrokenChangelog(Exception):
    pass


class Keyrings(collections.OrderedDict):
    def __init__(self):
        super().__init__()
        self.missing = False
        for sname, fname in self._init_list:
            for sp in KEYRINGS_PATH:
                kr_path = os.path.join(sp, fname)
                if os.path.exists(kr_path):
                    self[sname] = kr_path
                    break
            else:
                self.missing = True

    def warn_missing(self):
        warn('Some keyrings are missing, please run `git deb get-keyrings`')

    _init_list = (
        ('debian', 'debian-keyring.gpg'),
        ('debian-maintainers', 'debian-maintainers.gpg'),
        ('debian-emeritus', 'emeritus-keyring.gpg'),
        ('debian-emeritus-pgp', 'emeritus-keyring.pgp'),
        ('debian-removed', 'removed-keys.gpg'),
        ('debian-removed-pgp', 'removed-keys.pgp'),
    )


if False:
    apt_pkg.init_config()
    def apt_proxy_for(hostname):
        sbs = urllib.parse.urlsplit(hostname)
        sbd = sbs._asdict()

        # priority order than matches man:apt.conf
        proxy_url = apt_pkg.config.get(
            'Acquire::{scheme}::Proxy::{netloc}'.format(**sbd))
        if not proxy_url:
            proxy_url = apt_pkg.config.get(
                'Acquire::{scheme}::Proxy'.format(**sbd))
        if proxy_url == 'DIRECT':
            proxy_url = None
        elif not proxy_url:
            envvar = sbs.scheme + '_proxy'
            proxy_url = os.environ.get(envvar)
        # May set the key to None, to hopefully prevent requests
        # from parsing the environment
        return {sbs.scheme: proxy_url}


class CacheControl:
    def __init__(self, path):
        self.path = path


class CacheForever(CacheControl):
    def load_condition(self, cf):
        return True


class MaxAge(CacheControl):
    def __init__(self, path, max_age):
        super().__init__(path)
        self.max_age = max_age

    def load_condition(self, cf):
        return time.time() < os.fstat(cf.fileno()).st_mtime + self.max_age


def http_caching(sess, cache_dir):
    try:
        import cachecontrol # pypi:CacheControl
        class DiskCache(cachecontrol.cache.BaseCache):
            def __path_of_key(self, key):
                return cache_dir + urllib.parse.quote(key, safe='')
            def get(self, key):
                try:
                    return open(self.__path_of_key(key), 'rb').read()
                except FileNotFoundError:
                    pass
            def set(self, key, value):
                debug('cache {} {}'.format(key, value))
                open(self.__path_of_key(key), 'wb').write(value)
            def delete(self, key):
                os.unlink(self.__path_of_key(key))
        caching = cachecontrol.CacheControlAdapter(cache=DiskCache())
    except ImportError:
        pass
    else:
        debug('Setting up caching')
        sess.mount('http://', caching)
        sess.mount('https://', caching)


class MissingSource(Exception):
    pass


def upload_precedence(fi):
    return fi.first_seen, fi.name, fi.archive_name, fi.path

def file_sha1(fi):
    sha1 = hashlib.sha1()
    while True:
        chunk = fi.read(CHUNK_SIZE)
        if not chunk:
            break
        sha1.update(chunk)
    return sha1.hexdigest()

def clean_uid(uid, kid):
    if IDENT_RE.match(uid):
        return uid
    elif '@' in uid:
        return '<{}>'.format(uid)
    elif kid in email_fallbacks:
        return uid + ' <{}>'.format(email_fallbacks[kid])
    else:
        raise ValueError(uid)


PathKey = collections.namedtuple('PathKey', 'archive_name path name')
def path_key(finfo):
    # only the path component can contain slashes
    return PathKey(finfo.archive_name, finfo.path, finfo.name)


def set_excl(d, k, v):
    if k in d:
        if d[k] != v:
            raise RuntimeError(k, v, d[k])
        return
    d[k] = v


class Snapshots:
    def __init__(self):
        # s.d.o versions are cached for 600s
        # s.d.o/file/sha1 is good for 10 days
        self.debsnap_dir = os.path.expanduser('~/.cache/debsnap/')
        os.makedirs(self.debsnap_dir + 'by-hash', exist_ok=True)
        os.makedirs(self.debsnap_dir + 'archive', exist_ok=True)
        os.makedirs(self.debsnap_dir + 'json', exist_ok=True)

        self.hash_of_path = {}

        self.http = sess = requests.Session()
        # May not work out of the box
        #sess.proxies = apt_proxy_for(SNAPSHOTS_BASE)
        #sess.proxies = dict(http='http://localhost:8123')  # Polipo
        sess.timeout = 15.
        sess.trust_env = False

        # Requests doesn't have good disk caching options right now
        #http_caching(sess, self.cache_dir)

    def _api_request(self, url, *, cache_control, **kwargs):
        try:
            cf = open(cache_control.path)
        except FileNotFoundError:
            pass
        else:
            if cache_control.load_condition(cf):
                #debug('cache hit ' + cache_control.path)
                return json.loads(cf.read(),
                    object_hook=lambda args: types.SimpleNamespace(**args))
            cf.close()
        resp = self.http.get(SNAPSHOTS_BASE + url, **kwargs)
        if resp.status_code >= 400:
            warn('HTTP error {} on {}'.format(resp.status_code, url))
            resp.raise_for_status()
        with open(cache_control.path, 'w') as cf:
            cf.write(resp.text)

        return resp.json(
            object_hook=lambda args: types.SimpleNamespace(**args))

    def _get_by_hash(self, fhash, size, old_cache_path):
        # by-hash is actually by-sha1 atm
        ref_path = self.debsnap_dir + 'by-hash/' + fhash
        try:
            st = os.stat(ref_path)
        except FileNotFoundError:
            pass
        else:
            if st.st_size == size:
                #debug('new cache hit ' + old_cache_path)
                return ref_path
            os.unlink(ref_path)

        try:
            fi = open(old_cache_path, 'rb')
        except FileNotFoundError:
            pass
        else:
            st = os.fstat(fi.fileno())
            if st.st_size == size and file_sha1(fi) == fhash:
                #debug('old cache hit' + old_cache_path)
                os.rename(old_cache_path, ref_path)
                return ref_path
            os.unlink(old_cache_path)

        # this api only accepts sha1, though dscs keep other hashes
        resp = self.http.get(SNAPSHOTS_BASE + 'file/' + fhash, stream=True)
        resp.raise_for_status()
        sha1 = hashlib.sha1()

        with tempfile.NamedTemporaryFile(
            dir=self.debsnap_dir, prefix='git-deb-', suffix='.download'
        ) as tf:
            for chunk in resp.iter_content(CHUNK_SIZE):
                sha1.update(chunk)
                tf.write(chunk)
            tf.flush()
            assert tf.tell() == size
            assert sha1.hexdigest() == fhash
            # Don't fsync, we'll use size checks
            os.rename(tf.name, ref_path)
            tf.delete = False
        return ref_path

    def _get_file(self, fhash, finfos):
        # ignoring archive_name and path
        old_cache_finfo = min(finfos, key=upload_precedence)
        old_cache_path = self.debsnap_dir + old_cache_finfo.name
        size = old_cache_finfo.size
        #debug(old_cache_path, size, finfos)
        ref_path = self._get_by_hash(fhash, size, old_cache_path)
        for finfo in finfos:
            assert '/' not in finfo.name
            assert '/' not in finfo.archive_name
            assert '..' not in finfo.path
            assert finfo.path[0] == '/'
            dpath = self.debsnap_dir + 'archive/' + finfo.archive_name + finfo.path
            assert os.path.relpath(dpath, self.debsnap_dir).startswith('archive/')
            os.makedirs(dpath, exist_ok=True)
            if not finfo.name.startswith(pkgname_quoted + '_'):
                # Empty gzip files tend to go by many names
                continue
            local_path = dpath + '/' + finfo.name
            try:
                os.link(ref_path, local_path)
            except FileExistsError:
                assert os.path.samefile(ref_path, local_path), (ref_path, local_path)
            if finfo.name.endswith('.dsc'):
                finfo.local_path = local_path

    def get_versions(self):
        # new to old, with version sort
        # may not match dates in case of backports and mistakes
        vinfo = self._api_request('mr/package/{}/'.format(pkgname_quoted),
            cache_control=MaxAge(
                self.debsnap_dir + 'json/' + pkgname_quoted + '_versions.json', 600))
        return [el.version for el in vinfo.result]

    def grab_srcfiles(self, ver):
        try:
            # Polipo doesn't understand no-args max-stale, so set it to 100 days
            srcinfo = self._api_request(
                'mr/package/{}/{}/srcfiles?fileinfo=1'.format(pkgname_quoted, ver),
                headers={'max-stale': '8640000'},
                cache_control=CacheForever(
                    self.debsnap_dir + 'json/' + pkgname_quoted
                    + '_' + ver + '.srcfiles.json'))
        except requests.HTTPError as err:
            if err.response.status_code == 404:
                # http://snapshot.debian.org/package/curl/6.0-1.1.1/
                # http://snapshot.debian.org/package/curl/7.23.1-3+ppc64/
                raise MissingSource(ver)
            raise

        dsc_finfos = set()
        dsc_finfos_by_hash = {}
        for fhash, finfos in vars(srcinfo.fileinfo).items():
            assert finfos
            if len(finfos) > 1:
                # some appear in multiple archives
                assert len(set((fi.size) for fi in finfos)) == 1, finfos
                # same extension?
                assert len(set((fi.name.split('.')[-1]) for fi in finfos)) == 1, finfos
            assert '/' not in fhash
            self._get_file(fhash, finfos)
            if finfos[0].name.endswith('.dsc'):
                dsc_finfos.update(finfos)
                dsc_finfos_by_hash[fhash] = finfos
            for finfo in finfos:
                set_excl(self.hash_of_path, path_key(finfo), fhash)
        if not dsc_finfos:
            warn('No signature file for {}'.format(ver))
            # http://snapshot.debian.org/package/sudo/1.3.1pl4-1/
            raise MissingSource(ver)
        dsc_finfos_by_cleartext = collections.defaultdict(list)
        for finfo in dsc_finfos:
            dsc_path = finfo.local_path
            for kr_name, kr_path in keyrings.items():
                gi = debian.deb822.GpgInfo.from_file(
                    dsc_path, keyrings=[kr_path])
                if not gi.valid():
                    continue
                finfo.gi = gi
                finfo.kr_name = kr_name
                break
            else:
                # On wget_1.5.3-3.1.dsc
                # ERRSIG 7D7C0636C76F38D2 20 2 01 1039606003 4
                # indicating problems with an ElGamal signature
                # gnupg 1.2.5-3 can verify it, 1.4.0-3 can't:
                # 2002-12-11 1039606003 0 3 0 20 2 01 576E100B518D2F1636B028053CB892502FA3BC2D
                # On sudo_1.6.2p2-2.2.dsc
                # ERRSIG 7D7C0636C76F38D2 20 3 01 1019775429 4
                # Those keys are compromised:
                # http://lists.gnupg.org/pipermail/gnupg-announce/2003q4/000160.html

                # Or missing keys
                # On grub_0.97-16.1~bpo.1.dsc
                # ERRSIG 6908386EC98FE2A1 17 2 01 1160038149 9
                if 'NO_PUBKEY' in gi and keyrings.missing:
                    keyrings.warn_missing()
                bail('No valid signature on {} {}'.format(dsc_path, gi))
            # See /usr/share/doc/gnupg/DETAILS.gz
            for sigtype in 'GOODSIG REVKEYSIG EXPKEYSIG'.split():
                try:
                    kid, uid, *extra = gi[sigtype]
                except KeyError:
                    continue
                try:
                    finfo.uid = clean_uid(uid, kid)
                except ValueError:
                    bail('Bad uid {} for {} on {}'.format(uid, kid, dsc_path))
                finfo.kid = kid
                finfo.sigtype = sigtype
                break
            else:
                bail('Not a good signature {}'.format(finfo.gi))
            # Getting (only) the cleartext is way too tricky,
            # hopefully this guarantees there is a single signature
            # and we're not getting junk outside the signed area
            cleartext = subprocess.check_output(
                ['gpg', '--decrypt', '--keyring', kr_path, '--', dsc_path],
                stderr=subprocess.DEVNULL)
            dsc_finfos_by_cleartext[cleartext].append(finfo)
        if len(dsc_finfos_by_cleartext) > 1:
            # http://snapshot.debian.org/package/file/4.17-5etch2/
            bail('Multiple dsc cleartexts for version {} {}'.format(
                ver, [fi.local_path for fi in dsc_finfos]))
        elif len(dsc_finfos_by_hash) > 1:
            debug('Multiple dscs for version {} {}'.format(
                ver, [fi.local_path for fi in dsc_finfos]))
        return SrcPkg(ver, dsc_finfos, cleartext)


def cmd_capabilities(args):
    print('*import')
    print('*option')
    print('*refspec refs/heads/*:refs/debian/{}/*'.format(remote_quoted))
    print()

depth = None
def cmd_option(args):
    global depth
    if args.name == 'depth':
        depth = int(args.value)
        if depth <= 0:
            # transport.c doesn't really check the range so do it
            # here.  Git infinity == 2**31-1 (it's a C thing)
            msg = 'Depth must be > 0'
            print('error ' + msg)
            # Even though the protocol has error reporting,
            # git ignores errors if we don't exit
            bail(msg)
        else:
            print('ok')
    elif args.name == 'verbosity':
        verbosity = int(args.value)
        if verbosity < 1:
            warn = ignore
        elif verbosity > 1:
            debug = printerr
        print('ok')
    else:
        print('unsupported')

def cmd_list(args):
    # TODO check for unchanged
    remote_ref = 'refs/heads/{}'.format(pkgname_quoted)
    print('? ' + remote_ref)
    print('@{} HEAD'.format(remote_ref))
    print()

def open_chardet(fname):
    fi = open(fname, 'rb')
    tw = io.TextIOWrapper(fi, 'utf8')
    try:
        tw.read(65536)
    except UnicodeDecodeError:
        tw.detach()
        del tw
        fi.seek(0)
    else:
        tw.seek(0)
        return tw
    # Now we only know it's not utf8
    guess = chardet.detect(fi.read(65536))
    if guess['confidence'] < .5:
        warn('Low-confidence guess on non-utf8 file {}'.format(fname))
    encoding = guess['encoding']
    fi.seek(0)
    return io.TextIOWrapper(fi, encoding)

def quote_ver(ver):
    return ver.replace(':', '%').replace('~', '_')

def tagname(ver):
    # escape : and ~. Other problem characters aren't valid for debian
    #return 'refs/tags/' + urllib.parse.quote(ver, safe='+')
    return 'refs/tags/' + quote_ver(ver)

def resolve_ref(ver):
    try:
        return subprocess.check_output(
            ['git', 'show-ref', '-s', '--', tagname(ver)]).decode()
    except subprocess.CalledProcessError:
        pass

def gpg_ts(ts):
    if 'T' in ts:
        return isodate.parse_datetime(ts).timestamp()
    else:
        return int(ts)


class SrcPkg:
    prev_ver = None
    authorship = None

    def __init__(self, version, dsc_finfos, cleartext):
        self.version = version
        self.dsc_finfos = dsc_finfos
        assert len(set(fi.name for fi in dsc_finfos)) == 1
        dsc_finfo = next(iter(dsc_finfos))
        self.desc = dsc_finfo.local_path
        # actually bytes, not text
        self.cleartext = cleartext
        self.dsc = debian.deb822.Dsc(cleartext)
        part_names = [fi['name'] for fi in self.dsc['Files']]
        assert not any('/' in pn for pn in part_names), part_names
        #self.native = '-' not in version
        self.native = len(part_names) == 1
        if self.native:
            assert len(part_names) == 1, (part_names, version)
            self.native_name, = part_names
            assert self.native_name.startswith(
                '{}_{}.tar.'.format(pkgname_quoted, version)), self.native_name
        else:
            try:
                self.orig_version, debian_revision = version.split('-')
            except ValueError:
                # Fail: atop_1.23.dsc
                assert '-' not in version
                warn('Non-native package without a debian revision {}'.format(version))
                self.orig_version = version
            self.orig_name = None
            self.comp_names = []
            for pn in part_names:
                if pn.startswith('{}_{}.orig-'.format(pkgname_quoted, self.orig_version)):
                    self.comp_names.append(pn)
                elif pn.startswith('{}_{}.orig.'.format(pkgname_quoted, self.orig_version)):
                    assert self.orig_name is None
                    self.orig_name = pn
                else:
                    assert re.match(r'^{}_{}\.(diff|debian)\.'.format(
                        re.escape(pkgname_quoted), re.escape(version)), pn), pn
            assert self.orig_name is not None, version
            dsc_path_key = path_key(dsc_finfo)
            self.orig_key = tuple(
                snaps.hash_of_path[dsc_path_key._replace(name=on)]
                for on in [self.orig_name] + self.comp_names)

    _unpack_cache = {}
    def unpack(self, dsc_path):
        assert not hasattr(self, 'xdir')
        xdir = tdir + '/x/' + self.version
        # Will fail on bad checksums, but not bad signatures
        # Signatures were checked in grab_srcfiles
        silent_call(
            'dpkg-source -x --no-copy --'.split()
            + [dsc_path, xdir])
        self.xdir = xdir
        if not self.native:
            if self.orig_key in self._unpack_cache:
                self.orig_mtime, self.odir = self._unpack_cache[self.orig_key]
                return
            # This won't split component tarballs,
            # but it's good enough for a round trip
            odir = tdir + '/o/' + self.version
            silent_call(
                'dpkg-source -x --no-copy --skip-debianization --'.split()
                + [dsc_path, odir])
            mtime = 0
            for (dn, dirs, files) in os.walk(odir):
                for fn in files:
                    mtime1 = os.stat(os.path.join(dn, fn)).st_mtime
                    if mtime1 > mtime:
                        mtime = mtime1
            self.orig_mtime = mtime
            self.odir = odir
            self._unpack_cache[self.orig_key] = mtime, odir

    def parse_changelog(self, skip_versions):
        with open_chardet(self.xdir + '/debian/changelog') as cl:
            self.authorship, cl_vers = gitdeb.parse_changelog(cl, skip_versions)
        return cl_vers


def slurp_tree(path, desc):
    os.environ['GIT_WORK_TREE'] = path
    os.environ['GIT_INDEX_FILE'] = path + '.index'
    try:
        silent_call('git add -A'.split())
    except subprocess.CalledProcessError:
        bail('Error adding from {}'.format(desc))
    tree_hash = subprocess.check_output(
        'git write-tree'.split()).decode().rstrip()
    return tree_hash


def text_data(text):
    print('data {}'.format(len(text.encode())))
    print(text)


def binary_data(data):
    print('data {}'.format(len(data)))
    sys.stdout.flush()
    sys.stdout.buffer.write(data + b'\n')
    sys.stdout.buffer.flush()


done_refs = set()
def cmd_import(args):
    remote_ref = 'refs/heads/{}'.format(pkgname_quoted)
    assert args.refname in {remote_ref, 'HEAD'}, (args.refname, remote_ref)
    if remote_ref in done_refs:
        return
    import_ref = 'refs/debian/{}/{}'.format(remote_quoted, pkgname_quoted)
    orig_ref = 'refs/upstream/{}/{}'.format(remote_quoted, pkgname_quoted)
    versions = snaps.get_versions()
    assert not any('/' in ver for ver in versions)
    for ver in skip:
        versions.remove(ver)
    #debug(versions)
    version_set = set(versions)
    if depth:
        versions_depth = versions[:depth]
    else:
        versions_depth = versions
    versions_depth_set = set(versions_depth)
    resolved = {}
    pkg_by_ver = {}
    successors = collections.defaultdict(list)
    skip_versions = set()
    ghosts = set()
    is_root = True
    todo = set()
    nonnat_pkgs = []
    os.mkdir(tdir + '/x')
    os.mkdir(tdir + '/o')
    for ver in reversed(versions_depth):
        chash = resolve_ref(ver)
        if chash is not None:
            resolved[ver] = chash
            is_root = False
            continue
        progress('Downloading {}'.format(ver))
        try:
            pkg = pkg_by_ver[ver] = snaps.grab_srcfiles(ver)
            # order isn't important
            dsc_path = next(iter(pkg.dsc_finfos)).local_path
        except MissingSource:
            warn('Version {} has no source package'.format(ver))
            version_set.remove(ver)
            continue
        pkg.unpack(dsc_path)
        if not pkg.native:
            nonnat_pkgs.append(pkg)
        try:
            cl_vers = pkg.parse_changelog(skip_versions=skip_versions)
        except BrokenChangelog:
            # XXX Will still be added but outside the history graph
            warn("Couldn't parse changelog for {}".format(dsc_path))
            continue
        except UnicodeDecodeError:
            # This error will likely break newer uploads as well; bail
            bail("Couldn't parse changelog encoding for {}"
                 .format(dsc_path))
        assert cl_vers[0] == ver, (cl_vers[0], ver)
        skip_versions.add(ver)
        ver_ghosts = set()
        for ver1 in cl_vers[1:]:
            if ver1 in version_set:
                pkg.prev_ver = ver1
                successors[ver1].append(ver)
                if depth and ver1 not in versions_depth_set:
                    todo.add(ver)
                elif ver1 in resolved:
                    todo.add(ver)
                break
            elif ver1 not in ghosts and ver1 not in skip:
                ver_ghosts.add(ver1)
        else:
            todo.add(ver)
            if not is_root:
                warn('Version {} has no predecessor'.format(ver))
        if ver_ghosts:
            warn('Found ghost versions {}'.format(' '.join(ver_ghosts)))
            ghosts.update(ver_ghosts)
        is_root = False
    if not todo:
        debug('Already up to date')
        done_refs.add(remote_ref)
        return

    marks = itertools.count(1)
    prev_key = None
    for pkg in nonnat_pkgs:
        if pkg.orig_key == prev_key:
            pkg.orig_mark_ref = None
            continue
        orig_hash = slurp_tree(pkg.odir, desc=pkg.orig_name)
        pkg.orig_mark_ref = ':{}'.format(next(marks))
        print('commit ' + orig_ref)
        print('mark ' + pkg.orig_mark_ref)
        print('committer <tar-importer> {} +0000'.format(int(pkg.orig_mtime)))
        text_data('Import {}'.format(pkg.orig_version))
        print('deleteall')
        print('M 040000 {} '.format(orig_hash))
        print()
        prev_key = pkg.orig_key
        prev_pkg = pkg

    done = set()
    while todo:
        ver = todo.pop()
        if ver in done:
            bail('Changelog loop detected at {} {}, giving up'
                 .format(ver, pkg.prev_ver))
        progress('Importing {}'.format(ver))
        todo.update(successors[ver])
        pkg = pkg_by_ver[ver]
        tree_hash = slurp_tree(pkg.xdir, desc=pkg.desc)

        mark_ref = ':{}'.format(next(marks))
        print('commit ' + import_ref)
        print('mark ' + mark_ref)
        if pkg.authorship:
            author, date = pkg.authorship
            print(
                'committer {} {} +0000'.format(author, int(date.timestamp())))
        else:
            print('committer <malformed-changelog> 0 +0000')
        text_data('Import {}'.format(ver))
        if pkg.prev_ver is not None:
            if not depth or pkg.prev_ver in versions_depth_set:
                print('from ' + resolved[pkg.prev_ver])
        if not pkg.native and pkg.orig_mark_ref is not None:
            print('merge ' + pkg.orig_mark_ref)
        print('deleteall')
        print('M 040000 {} '.format(tree_hash))
        print()
        print('reset ' + tagname(ver))
        print('from ' + mark_ref)
        assert ver not in resolved, ver
        resolved[ver] = mark_ref
        for dsc_finfo in pkg.dsc_finfos:
            msg = 'Upload {}'.format(ver)
            if (dsc_finfo.kr_name, dsc_finfo.sigtype) != ('debian', 'GOODSIG'):
                msg += ' ({kr_name}/{kid} {sigtype})'.format(**vars(dsc_finfo))
            (fprint, sig_date, sig_ts, exp_ts, sigver, reserved, pkalg,
             hashalg, sigclass, fprint1, *extra) = dsc_finfo.gi['VALIDSIG']
            #sigid, date1, ts1, *extra = gi['SIG_ID']
            print('tag {}/{}'.format(dsc_finfo.archive_name, quote_ver(ver)))
            print('from ' + mark_ref)
            print('tagger {} {} +0000'.format(dsc_finfo.uid, gpg_ts(sig_ts)))
            msg += '\n\n'
            with open(dsc_finfo.local_path, 'rb') as df:
                binary_data(msg.encode() + df.read())
        done.add(ver)
    print('reset ' + import_ref)
    print('from ' + resolved[versions[0]])
    done_refs.add(remote_ref)


email_fallbacks = {}
skip = set()
def parse_url_debsnap(url):
    global email_fallbacks, skip
    # use snapshot.debian.org
    distribution = 'debian'
    if not url.query:
        return

    # grub?skip=0.97-16.1~bpo.1
    # sudo?skip=1.6.2p2-2.2
    # git clone deb::gnupg?skip=1.4.6-1~bpo.1,1.4.6-2.1 gnupg
    urlparams = urllib.parse.parse_qs(
        url.query.replace('+', '%2B'),
        strict_parsing=True, errors='strict')

    assert {'skip', 'trust', 'email'}.issuperset(urlparams.keys()), urlparams
    if 'skip' in urlparams:
        skip = set(sum((el.split(',') for el in urlparams['skip']), []))
    if 'trust' in urlparams:
        os.mkdir(tdir + '/k')
        for kid in set(sum((el.split(',') for el in urlparams['trust']), [])):
            with open(os.path.join(tdir, 'k', kid + '.gpg'), 'w') as krf:
                subprocess.check_call(['gpg', '--export', '--', kid], stdout=krf)
            keyrings['local:' + kid] = krf.name
    if 'email' in urlparams:
        for emap in set(sum((el.split(',') for el in urlparams['email']), [])):
            kid, eml = IDENT_RE.match(emap).groups()
            assert SIG_KID_RE.match(kid), kid
            # Check the key exists?
            email_fallbacks[kid] = eml

def parse_url_launchpad(url):
    # use launchpad
    # the launchpad dataset is usable enough once you know the right api,
    # but already exposed as bzr branches
    # not sure how these deal with the orig/debian/patches distinction
    # it's less granular for debian, probably because sync is done by cronjob
    if url.query:
        urlparams = urllib.parse.parse_qs(
            url.query, strict_parsing=True, errors='strict')
        assert urlparams.keys() in {'distribution', 'archive'}, urlparams
    else:
        urlparams = {}

    distribution = urlparams.get('distribution', 'ubuntu')
    archive = urlparams.get('archive', 'primary')
    assert distribution in {'debian', 'ubuntu'}, distribution
    assert archive in {'primary', 'partner'}, archive


def protocol_main():
    parser1 = argparse.ArgumentParser()
    commands = parser1.add_subparsers(dest='command', metavar='command')
    sp_capabilities = commands.add_parser('capabilities')
    sp_capabilities.set_defaults(action=cmd_capabilities)
    sp_option = commands.add_parser('option')
    sp_option.set_defaults(action=cmd_option)
    sp_option.add_argument('name')
    sp_option.add_argument('value')
    sp_list = commands.add_parser('list')
    sp_list.set_defaults(action=cmd_list)
    sp_import = commands.add_parser('import')
    sp_import.set_defaults(action=cmd_import)
    sp_import.add_argument('refname')

    import_batch = False

    for line in sys.stdin:
        assert line[-1] == '\n'
        line1 = line[:-1]
        if not line1:
            if import_batch:
                import_batch = False
                print('done', flush=True)
                continue
            else:
                break
        debug(line1)
        args = parser1.parse_args(line1.split())
        if args.command == 'import':
            if not import_batch:
                print('feature done')
                import_batch = True
        else:
            assert not import_batch
        args.action(args)
        sys.stdout.flush()

def main():
    global pkgname_quoted, remote_quoted, keyrings, snaps
    parser0 = argparse.ArgumentParser()
    parser0.add_argument('remote')
    parser0.add_argument('url')
    args0 = parser0.parse_args()
    remote_quoted = urllib.parse.quote(args0.remote, safe='')
    url = urllib.parse.urlsplit(args0.url, scheme='deb')
    assert url.scheme == 'deb'
    assert not url.netloc
    assert not url.fragment

    keyrings = Keyrings()
    parse_url_debsnap(url)

    # Run after ?trust= has been parsed
    if not keyrings:
        bail('No keyrings are available, please run `git deb get-keyrings`')
    elif keyrings.missing:
        keyrings.warn_missing()

    pkgname = urllib.parse.unquote(url.path, errors='strict')
    pkgname_quoted = urllib.parse.quote(pkgname, safe='')
    assert pkgname == pkgname_quoted, (pkgname, pkgname_quoted)

    snaps = Snapshots()
    protocol_main()

with contextlib.ExitStack() as estack:
    tdir = estack.enter_context(
        tempfile.TemporaryDirectory(prefix='git-deb-', suffix='.tmp'))
    main()

