#!/usr/bin/python3.3
"""
deb:pkgname

Sadly, git parses the above as ssh pseudo-urls.
Workaround:
deb::deb:pkgname deb::pkgname
"""

import apt_pkg # pypi:python-apt, apt:python3-apt
import argparse
import chardet # pypi:chardet
import collections
import debian.deb822 # pypi:python-apt
import email.utils
import io
import isodate # pypi:isodate
import json
import os
import re
import requests # pypi
import subprocess
import sys
import tempfile
import time
import types
import urllib.parse


SNAPSHOTS_BASE = 'http://snapshot.debian.org/'
KEYRINGS = collections.OrderedDict([
    ('debian', '/usr/share/keyrings/debian-keyring.gpg'),
    ('debian-maintainers', '/usr/share/keyrings/debian-maintainers.gpg'),
    ('debian-emeritus', os.path.expanduser('~/var/co/git-bzr/debian-keyring/output/keyrings/emeritus-keyring.gpg')),
    ('debian-emeritus-pgp', os.path.expanduser('~/var/co/git-bzr/debian-keyring/output/keyrings/emeritus-keyring.pgp')),
    ('debian-removed', os.path.expanduser('~/var/co/git-bzr/debian-keyring/output/keyrings/removed-keys.gpg')),
    ('debian-removed-pgp', os.path.expanduser('~/var/co/git-bzr/debian-keyring/output/keyrings/removed-keys.pgp')),
])

# very relaxed, we'll be dealing with historical data
VERSION_LINE_RE = re.compile(r'^[a-z0-9][a-z0-9+.-]*\s+\(([^ ]+)\)')
AUTHOR_LINE_RE = re.compile(r'^ --\s*([^<>]*<[^<>]+>)  (.*)$')

# I've no idea
CHUNK_SIZE = 65536


class JSONEncoder(json.JSONEncoder):
    def default(self, o):
        if isinstance(o, types.SimpleNamespace):
            return vars(o)
        return super().default(o)

def json_dump(val):
    return json.dumps(val, cls=JSONEncoder)


def apt_proxy_for(hostname):
    sbs = urllib.parse.urlsplit(hostname)
    sbd = sbs._asdict()

    # priority order than matches man:apt.conf
    proxy_url = apt_pkg.config.get(
        'Acquire::{scheme}::Proxy::{netloc}'.format(**sbd))
    if not proxy_url:
        proxy_url = apt_pkg.config.get(
            'Acquire::{scheme}::Proxy'.format(**sbd))
    if proxy_url == 'DIRECT':
        proxy_url = None
    elif not proxy_url:
        envvar = sbs.scheme + '_proxy'
        proxy_url = os.environ.get(envvar)
    # May set the key to None, to hopefully prevent requests
    # from parsing the environment
    return {sbs.scheme: proxy_url}


class CacheControl:
    def __init__(self, path):
        self.path = path


class CacheForever(CacheControl):
    def load_condition(self, cf):
        return True


class MaxAge(CacheControl):
    def __init__(self, path, max_age):
        super().__init__(path)
        self.max_age = max_age

    def load_condition(self, cf):
        return time.time() < os.fstat(cf.fileno()).st_mtime + self.max_age


def http_caching(sess, cache_dir):
    try:
        import cachecontrol # pypi:CacheControl
        class DiskCache(cachecontrol.cache.BaseCache):
            def __path_of_key(self, key):
                return cache_dir + urllib.parse.quote(key, safe='')
            def get(self, key):
                try:
                    return open(self.__path_of_key(key), 'rb').read()
                except FileNotFoundError:
                    pass
            def set(self, key, value):
                print('cache {} {}'.format(key, value), file=sys.stderr)
                open(self.__path_of_key(key), 'wb').write(value)
            def delete(self, key):
                os.unlink(self.__path_of_key(key))
        caching = cachecontrol.CacheControlAdapter(cache=DiskCache())
    except ImportError:
        pass
    else:
        print('Setting up caching', file=sys.stderr)
        sess.mount('http://', caching)
        sess.mount('https://', caching)


class MissingSource(Exception):
    pass


class Snapshots:
    def __init__(self):
        # s.d.o versions are cached for 600s
        # s.d.o/file/sha1 is good for 10 days
        self.cache_dir = os.path.expanduser('~/.cache/git-remote-deb/')
        self.debsnap_dir = os.path.expanduser('~/.cache/debsnap/')

        self.http = sess = requests.Session()
        # May not work out of the box
        #sess.proxies = apt_proxy_for(SNAPSHOTS_BASE)
        #sess.proxies = dict(http='http://localhost:8123')  # Polipo
        sess.timeout = 15.
        sess.trust_env = False

        # Requests doesn't have good disk caching options right now
        #http_caching(sess, self.cache_dir)

    def _api_request(self, url, *, cache_control, **kwargs):
        try:
            cf = open(cache_control.path)
        except FileNotFoundError:
            pass
        else:
            if cache_control.load_condition(cf):
                #print('cache hit ' + cache_control.path, file=sys.stderr)
                return json.loads(cf.read(),
                    object_hook=lambda args: types.SimpleNamespace(**args))
            cf.close()
        resp = self.http.get(SNAPSHOTS_BASE + url, **kwargs)
        if resp.status_code >= 400:
            print('Request error on {}'.format(url), file=sys.stderr)
            resp.raise_for_status()
        with open(cache_control.path, 'w') as cf:
            cf.write(resp.text)

        return resp.json(
            object_hook=lambda args: types.SimpleNamespace(**args))

    def _get_file(self, fhash, finfo, extra_names):
        # ignoring archive_name and path
        name = finfo.name
        size = finfo.size
        path = self.debsnap_dir + name
        #print(name, size, finfo, extra_names, file=sys.stderr)
        try:
            st = os.stat(path)
        except FileNotFoundError:
            pass
        else:
            # Check for partial downloads, anything else will
            # be caught by dscverify
            if st.st_size == size:
                #print('skipping {}'.format(name), file=sys.stderr)
                return path
            os.unlink(path)
        resp = self.http.get(SNAPSHOTS_BASE + 'file/' + fhash, stream=True)
        resp.raise_for_status()
        ts = finfo.first_seen_ts
        try:
            fd, tn = tempfile.mkstemp(dir=self.debsnap_dir)
            for chunk in resp.iter_content(CHUNK_SIZE):
                os.write(fd, chunk)
            os.utime(fd, times=(ts, ts))
            os.rename(tn, path)
            for ename in extra_names:
                os.link(path, self.debsnap_dir + ename)
        except:
            os.unlink(tn)
            raise
        finally:
            os.close(fd)
        return path

    def get_versions(self):
        # new to old, with version sort
        # may not match dates in case of backports and mistakes
        vinfo = self._api_request('mr/package/{}/'.format(pkgname_quoted),
            cache_control=MaxAge(
                self.cache_dir + pkgname_quoted + '_versions.json', 600))
        return [el.version for el in vinfo.result]

    def grab_srcfiles(self, ver):
        try:
            # Polipo doesn't understand no-args max-stale, so set it to 100 days
            srcinfo = self._api_request(
                'mr/package/{}/{}/srcfiles?fileinfo=1'.format(pkgname_quoted, ver),
                headers={'max-stale': '8640000'},
                cache_control=CacheForever(
                    self.cache_dir + pkgname_quoted
                    + '_' + ver + '.srcfiles.json'))
        except requests.HTTPError:
            raise MissingSource(ver)

        # sha1
        #hashes = [el.hash for el in srcinfo.result]
        dsc_finfo = None
        for fhash, finfos in vars(srcinfo.fileinfo).items():
            assert finfos
            if len(finfos) > 1:
                # some appear in multiple archives
                assert len(set((fi.size) for fi in finfos)) == 1, finfos
            # lex sort is fine here
            finfo = min(finfos, key=lambda fi: (
                fi.first_seen, fi.name, fi.archive_name, fi.path))
            first_seen_dt = isodate.parse_datetime(finfo.first_seen)
            finfo.first_seen_ts = first_seen_dt.timestamp()
            # Some orig.tar.gz have multiple names :(
            extra_names = set(fi.name for fi in finfos if fi.name != finfo.name)
            # this api only accepts sha1, though dscs keep other hashes
            path = self._get_file(fhash, finfo, extra_names)
            if finfo.name.endswith('.dsc'):
                # Curl again. Horrors.
                assert dsc_finfo is None, json_dump(srcinfo.fileinfo)
                dsc_finfo = finfo
                dsc_path = path
        assert dsc_finfo
        # Slow
        subprocess.check_call(
            ['dscverify', '--no-sig-check', dsc_path],
            stdout=subprocess.DEVNULL)
        for kr_name, kr_path in KEYRINGS.items():
            gi = debian.deb822.GpgInfo.from_file(dsc_path, keyrings=[kr_path])
            if not gi.valid():
                continue
            dsc_finfo.gi = gi
            dsc_finfo.kr_name = kr_name
            break
        else:
            # For example, on wget_1.5.3-3.1.dsc
            # ERRSIG 7D7C0636C76F38D2 20 2 01 1039606003 4
            # indicating problems with an ElGamal signature
            bail('No valid signature on {} {}'.format(dsc_path, gi))
        return dsc_finfo, dsc_path


def cmd_capabilities(args):
    print('*import')
    print('*option')
    print('*refspec refs/heads/*:refs/debian/{}/*'.format(remote_quoted))
    print()

def bail(msg):
    print(msg, file=sys.stderr)
    sys.exit(1)

depth = None
def cmd_option(args):
    global depth
    if args.name == 'depth':
        depth = int(args.value)
        if depth <= 0:
            # transport.c doesn't really check,
            # depth=0 might mean infinity in git internals
            msg = 'Depth must be > 0'
            print('error ' + msg)
            # Even though the protocol has error reporting,
            # git ignores errors if we don't exit
            bail(msg)
        else:
            print('ok')
    else:
        print('unsupported')

def cmd_list(args):
    # TODO check for unchanged
    remote_ref = 'refs/heads/{}'.format(pkgname_quoted)
    print('? ' + remote_ref)
    print('@{} HEAD'.format(remote_ref))
    print()

def parse_changelog(cl, skip_versions):
    top_entry = True
    within = False
    versions = []
    for line in cl:
        line = line.rstrip()
        if not line:
            continue
        if not within:
            match = VERSION_LINE_RE.match(line)
            if not match:
                print('Giving up on changelog {!r}'.format(line), file=sys.stderr)
                break
            ver1, = match.groups()
            versions.append(ver1)
            if not top_entry and ver1 in skip_versions:
                break
            within = True
        else:
            match = AUTHOR_LINE_RE.match(line)
            if not match:
                continue
            if top_entry:
                author, date = match.groups()
                date = email.utils.parsedate_to_datetime(date)
            top_entry = False
            within = False
    if top_entry:
        # There was no valid changelog stanza
        return None, None
    return (author, date), versions

def open_chardet(fname):
    fi = open(fname, 'rb')
    guess = chardet.detect(fi.read(65536))
    fi.seek(0)
    if guess['confidence'] < .5 or guess['encoding'] == 'ascii':
        # Debian standard
        encoding = 'utf8'
    else:
        encoding = guess['encoding']
    return io.TextIOWrapper(fi, encoding)

def tagname(ver):
    # escape : and ~. Other problem characters aren't valid for debian
    #return 'refs/tags/' + urllib.parse.quote(ver, safe='+')
    return 'refs/tags/' + ver.replace(':', '%').replace('~', '_')

def gpg_ts(ts):
    if 'T' in ts:
        return isodate.parse_datetime(ts).timestamp()
    else:
        return int(ts)

def cmd_import(args):
    remote_ref = 'refs/heads/{}'.format(pkgname_quoted)
    assert args.refname in {remote_ref, 'HEAD'}, (args.refname, remote_ref)
    if remote_ref in done_refs:
        return
    import_ref = 'refs/debian/{}/{}'.format(remote_quoted, pkgname_quoted)
    versions = snaps.get_versions()
    assert not any('/' in ver for ver in versions)
    #print(versions, file=sys.stderr)
    version_set = set(versions)
    if depth:
        versions = versions[:depth]
    authorships = {}
    first_seen = {}
    predecessors = {}
    successors = collections.defaultdict(list)
    skip_versions = set()
    is_root = True
    todo = set()
    with tempfile.TemporaryDirectory() as tdir:
        for ver in reversed(versions):
            try:
                dsc_finfo, dsc_path = snaps.grab_srcfiles(ver)
            except MissingSource:
                # http://snapshot.debian.org/package/curl/6.0-1.1.1/
                print(
                    'Version {} has no source package'.format(ver),
                    file=sys.stderr)
                version_set.remove(ver)
                continue
            first_seen[ver] = dsc_finfo.first_seen
            xdir = tdir + '/' + ver
            subprocess.call(
                'dpkg-source -x --no-check --'.split() + [dsc_path, xdir],
                stdout=subprocess.DEVNULL)
            with open_chardet(xdir + '/debian/changelog') as cl:
                authorship, cl_vers = parse_changelog(cl, skip_versions)
            if not authorship:
                continue
            assert cl_vers[0] == ver, (cl_vers[0], ver)
            skip_versions.add(ver)
            for ver1 in cl_vers[1:]:
                if ver1 in version_set:
                    predecessors[ver] = ver1
                    successors[ver1].append(ver)
                    break
                else:
                    print('Found ghost version {}'.format(ver1), file=sys.stderr)
            else:
                todo.add(ver)
                if not is_root:
                    print('Version {} has no predecessor'.format(ver), file=sys.stderr)
            authorships[ver] = authorship
            is_root = False
        assert todo
        done = set()
        while todo:
            ver = todo.pop()
            if ver in done:
                bail('Changelog loop detected at {} {}, giving up'
                     .format(ver, predecessors[ver]))
            todo.update(successors[ver])
            dsc_finfo, dsc_path = snaps.grab_srcfiles(ver)
            xdir = tdir + '/' + ver
            os.environ['GIT_WORK_TREE'] = xdir
            os.environ['GIT_INDEX_FILE'] = xdir + '.index'
            subprocess.check_call('git add -A'.split())
            tree_hash = subprocess.check_output(
                'git write-tree'.split()).decode().rstrip()

            # See /usr/share/doc/gnupg/DETAILS.gz
            for sigtype in 'GOODSIG REVKEYSIG'.split():
                try:
                    kid, uid, *extra = dsc_finfo.gi[sigtype]
                    break
                except KeyError:
                    pass
            else:
                bail('Not a good signature {}'.format(dsc_finfo.gi))
            (fprint, sig_date, sig_ts, exp_ts, sigver, reserved, pkalg,
             hashalg, sigclass, fprint1, *extra) = dsc_finfo.gi['VALIDSIG']
            #sigid, date1, ts1, *extra = gi['SIG_ID']

            print('commit ' + import_ref)
            print('mark :1')
            if ver in authorships:
                author, date = authorships[ver]
                print(
                    'author {} {} +0000'.format(author, int(date.timestamp())))
                print('committer {} {} +0000'.format(uid, gpg_ts(sig_ts)))
            else:
                print(
                    'committer <malformed-changelog> {} +0000'
                    .format(int(dsc_finfo.first_seen_ts)))
            msg = 'Import {}'.format(ver)
            if (dsc_finfo.kr_name, sigtype) != ('debian', 'GOODSIG'):
                msg += ' ({}/{} {})'.format(dsc_finfo.kr_name, kid, sigtype)
            print('data {}'.format(len(msg.encode())))
            print(msg)
            if ver in predecessors:
                print('from ' + tagname(predecessors[ver]))
            print('deleteall')
            print('M 040000 {} '.format(tree_hash))
            print()
            print('reset ' + tagname(ver))
            print('from :1')
            done.add(ver)
    done_refs.add(remote_ref)

apt_pkg.init_config()
snaps = Snapshots()

parser0 = argparse.ArgumentParser()
parser0.add_argument('remote')
parser0.add_argument('url')
args0 = parser0.parse_args()
remote_quoted = urllib.parse.quote(args0.remote, safe='')
url = urllib.parse.urlsplit(args0.url, scheme='deb')
assert url.scheme == 'deb'
assert not url.netloc
assert not url.fragment

if True:
    # use snapshot.debian.org
    assert not url.query
    distribution = 'debian'
else:
    # use launchpad
    # the launchpad dataset is usable enough once you know the right api,
    # but already exposed as bzr branches
    # not sure how these deal with the orig/debian/patches distinction
    # it's less granular for debian, probably because sync is done by cronjob
    if url.query:
        urlparams = urllib.parse.parse_qs(
            url.query, strict_parsing=True, errors='strict')
        assert urlparams.keys() in {'distribution', 'archive'}, urlparams
    else:
        urlparams = {}

    distribution = urlparams.get('distribution', 'ubuntu')
    archive = urlparams.get('archive', 'primary')
    assert distribution in {'debian', 'ubuntu'}, distribution
    assert archive in {'primary', 'partner'}, archive

pkgname = urllib.parse.unquote(url.path, errors='strict')
pkgname_quoted = urllib.parse.quote(pkgname, safe='')

assert pkgname == pkgname_quoted, (pkgname, pkgname_quoted)

parser1 = argparse.ArgumentParser()
commands = parser1.add_subparsers(dest='command', metavar='command')
sp_capabilities = commands.add_parser('capabilities')
sp_capabilities.set_defaults(action=cmd_capabilities)
sp_option = commands.add_parser('option')
sp_option.set_defaults(action=cmd_option)
sp_option.add_argument('name')
sp_option.add_argument('value')
sp_list = commands.add_parser('list')
sp_list.set_defaults(action=cmd_list)
sp_import = commands.add_parser('import')
sp_import.set_defaults(action=cmd_import)
sp_import.add_argument('refname')

import_batch = False
done_refs = set()

for line in sys.stdin:
    assert line[-1] == '\n'
    line1 = line[:-1]
    if not line1:
        if import_batch:
            import_batch = False
            print('done', flush=True)
            continue
        else:
            break
    print(line1, file=sys.stderr)
    args = parser1.parse_args(line1.split())
    if args.command == 'import':
        if not import_batch:
            print('feature done')
            import_batch = True
    else:
        assert not import_batch
    args.action(args)
    sys.stdout.flush()

